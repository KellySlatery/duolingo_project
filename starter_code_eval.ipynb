{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: English to Spanish Morpholgical Order of Acqusition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kelly Slatery | US-DSI-10 | 03.13.2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from io import open\n",
    "import math\n",
    "import os\n",
    "\n",
    "from future.builtins import range\n",
    "from future.utils import iterkeys, iteritems\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Evaluates your predictions. This loads the dev labels and your predictions, and then evaluates them, printing the\n",
    "    results for a variety of metrics to the screen.\n",
    "    \"\"\"\n",
    "\n",
    "    test_metrics()\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Duolingo shared task evaluation script')\n",
    "    parser.add_argument('--pred', help='Predictions file name', required=True)\n",
    "    parser.add_argument('--key', help='Labelled keys', required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    assert os.path.isfile(args.pred)\n",
    "\n",
    "    print('\\nLoading labels for exercises...')\n",
    "    labels = load_labels(args.key)\n",
    "\n",
    "    print('Loading predictions for exercises...')\n",
    "    predictions = load_labels(args.pred)\n",
    "\n",
    "    actual = []\n",
    "    predicted = []\n",
    "\n",
    "    for instance_id in iterkeys(labels):\n",
    "        try:\n",
    "            actual.append(labels[instance_id])\n",
    "            predicted.append(predictions[instance_id])\n",
    "        except KeyError:\n",
    "            print('No prediction for instance ID ' + instance_id + '!')\n",
    "\n",
    "    metrics = evaluate_metrics(actual, predicted)\n",
    "    line = '\\t'.join([('%s=%.3f' % (metric, value)) for (metric, value) in iteritems(metrics)])\n",
    "    print('Metrics:\\t' + line)\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    \"\"\"\n",
    "    This loads labels, either the actual ones or your predictions.\n",
    "\n",
    "    Parameters:\n",
    "        filename: the filename pointing to your labels\n",
    "\n",
    "    Returns:\n",
    "        labels: a dict of instance_ids as keys and labels between 0 and 1 as values\n",
    "    \"\"\"\n",
    "    labels = dict()\n",
    "\n",
    "    with open(filename, 'rt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                line = line.split()\n",
    "            instance_id = line[0]\n",
    "            label = float(line[1])\n",
    "            labels[instance_id] = label\n",
    "    return labels\n",
    "\n",
    "\n",
    "def compute_acc(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of your predictions, using 0.5 as a cutoff.\n",
    "\n",
    "    Note that these inputs are lists, not dicts; they assume that actual and predicted are in the same order.\n",
    "\n",
    "    Parameters (here and below):\n",
    "        actual: a list of the actual labels\n",
    "        predicted: a list of your predicted labels\n",
    "    \"\"\"\n",
    "    num = len(actual)\n",
    "    acc = 0.\n",
    "    for i in range(num):\n",
    "        if round(actual[i], 0) == round(predicted[i], 0):\n",
    "            acc += 1.\n",
    "    acc /= num\n",
    "    return acc\n",
    "\n",
    "\n",
    "def compute_avg_log_loss(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the average log loss of your predictions.\n",
    "    \"\"\"\n",
    "    num = len(actual)\n",
    "    loss = 0.\n",
    "\n",
    "    for i in range(num):\n",
    "        p = predicted[i] if actual[i] > .5 else 1. - predicted[i]\n",
    "        loss -= math.log(p)\n",
    "    loss /= num\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_auroc(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the area under the receiver-operator characteristic curve.\n",
    "    This code a rewriting of code by Ben Hamner, available here:\n",
    "    https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/auc.py\n",
    "    \"\"\"\n",
    "    num = len(actual)\n",
    "    temp = sorted([[predicted[i], actual[i]] for i in range(num)], reverse=True)\n",
    "\n",
    "    sorted_predicted = [row[0] for row in temp]\n",
    "    sorted_actual = [row[1] for row in temp]\n",
    "\n",
    "    sorted_posterior = sorted(zip(sorted_predicted, range(len(sorted_predicted))))\n",
    "    r = [0 for k in sorted_predicted]\n",
    "    cur_val = sorted_posterior[0][0]\n",
    "    last_rank = 0\n",
    "    for i in range(len(sorted_posterior)):\n",
    "        if cur_val != sorted_posterior[i][0]:\n",
    "            cur_val = sorted_posterior[i][0]\n",
    "            for j in range(last_rank, i):\n",
    "                r[sorted_posterior[j][1]] = float(last_rank+1+i)/2.0\n",
    "            last_rank = i\n",
    "        if i==len(sorted_posterior)-1:\n",
    "            for j in range(last_rank, i+1):\n",
    "                r[sorted_posterior[j][1]] = float(last_rank+i+2)/2.0\n",
    "\n",
    "    num_positive = len([0 for x in sorted_actual if x == 1])\n",
    "    num_negative = num - num_positive\n",
    "    sum_positive = sum([r[i] for i in range(len(r)) if sorted_actual[i] == 1])\n",
    "    auroc = ((sum_positive - num_positive * (num_positive + 1) / 2.0) / (num_negative * num_positive))\n",
    "\n",
    "    return auroc\n",
    "\n",
    "\n",
    "def compute_f1(actual, predicted):\n",
    "    \"\"\"\n",
    "    Computes the F1 score of your predictions. Note that we use 0.5 as the cutoff here.\n",
    "    \"\"\"\n",
    "    num = len(actual)\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    for i in range(num):\n",
    "        if actual[i] >= 0.5 and predicted[i] >= 0.5:\n",
    "            true_positives += 1\n",
    "        elif actual[i] < 0.5 and predicted[i] >= 0.5:\n",
    "            false_positives += 1\n",
    "        elif actual[i] >= 0.5 and predicted[i] < 0.5:\n",
    "            false_negatives += 1\n",
    "        else:\n",
    "            true_negatives += 1\n",
    "\n",
    "    try:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        F1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        F1 = 0.0\n",
    "\n",
    "    return F1\n",
    "\n",
    "\n",
    "def evaluate_metrics(actual, predicted):\n",
    "    \"\"\"\n",
    "    This computes and returns a dictionary of notable evaluation metrics for your predicted labels.\n",
    "    \"\"\"\n",
    "    acc = compute_acc(actual, predicted)\n",
    "    avg_log_loss = compute_avg_log_loss(actual, predicted)\n",
    "    auroc = compute_auroc(actual, predicted)\n",
    "    F1 = compute_f1(actual, predicted)\n",
    "\n",
    "    return {'accuracy': acc, 'avglogloss': avg_log_loss, 'auroc': auroc, 'F1': F1}\n",
    "\n",
    "\n",
    "def test_metrics():\n",
    "    actual = [1, 0, 0, 1, 1, 0, 0, 1, 0, 1]\n",
    "    predicted = [0.8, 0.2, 0.6, 0.3, 0.1, 0.2, 0.3, 0.9, 0.2, 0.7]\n",
    "    metrics = evaluate_metrics(actual, predicted)\n",
    "    metrics = {key: round(metrics[key], 3) for key in iterkeys(metrics)}\n",
    "    assert metrics['accuracy'] == 0.700\n",
    "    assert metrics['avglogloss'] == 0.613\n",
    "    assert metrics['auroc'] == 0.740\n",
    "    assert metrics['F1'] == 0.667\n",
    "    print('Verified that our environment is calculating metrics correctly.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
